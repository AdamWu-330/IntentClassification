{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0CVgrappZ2G",
        "outputId": "04ef87bf-53b7-49fe-e0fd-b70631c20b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from intent_classification_helper import *"
      ],
      "metadata": {
        "id": "WYruRt2ZqYSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.ones((2,2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAYUAwgcrrRG",
        "outputId": "c851720d-bf76-4479-87fa-e18a4ea367c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1.],\n",
              "       [1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/drive/MyDrive/nlp_datasets/CLINC150/clinc150_uci/data_full.json')\n",
        "data = json.load(f)\n",
        "\n",
        "train_df = pd.DataFrame.from_dict(data['train'])\n",
        "train_df.columns = ['message', 'intent']\n",
        "\n",
        "val_df = pd.DataFrame.from_dict(data['val'])\n",
        "val_df.columns = ['message', 'intent']\n",
        "\n",
        "test_df = pd.DataFrame.from_dict(data['test'])\n",
        "test_df.columns = ['message', 'intent']\n",
        "\n",
        "print('number of intent classes in training set: ', len(set(train_df['intent'])))\n",
        "print('number of intent classes in val set: ', len(set(val_df['intent'])))\n",
        "print('number of intent classes in test set: ', len(set(test_df['intent'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ta7LR2aqaS6",
        "outputId": "35f2c008-eed5-42ae-cc94-1202d7e04050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of intent classes in training set:  150\n",
            "number of intent classes in val set:  150\n",
            "number of intent classes in test set:  150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode label\n",
        "le = LabelEncoder()\n",
        "train_df['intent'] = le.fit_transform(train_df['intent'])\n",
        "val_df['intent'] = le.fit_transform(val_df['intent'])\n",
        "test_df['intent'] = le.fit_transform(test_df['intent'])\n",
        "\n",
        "onehot_encoder = OneHotEncoder()\n",
        "y_train = onehot_encoder.fit_transform(train_df['intent'].values.reshape(-1, 1)).todense()\n",
        "y_val = onehot_encoder.fit_transform(val_df['intent'].values.reshape(-1, 1)).todense()\n",
        "y_test = onehot_encoder.fit_transform(test_df['intent'].values.reshape(-1, 1)).todense()\n",
        "\n",
        "# preprocess text\n",
        "print('Preprocessing text on training set...')\n",
        "preprocess_text(train_df, 'message')\n",
        "\n",
        "print('Preprocessing text on val set...')\n",
        "preprocess_text(val_df, 'message')\n",
        "\n",
        "print('Preprocessing text on test set...')\n",
        "preprocess_text(test_df, 'message')\n",
        "\n",
        "x_train = train_df['message'].to_list()\n",
        "x_val = val_df['message'].to_list()\n",
        "x_test = test_df['message'].to_list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMXpNPAdvJFo",
        "outputId": "587ed85c-770a-42eb-fca1-d38d684e3ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing text on training set...\n",
            "\n",
            "\n",
            "Start text preprocessing: \n",
            "--------------------------\n",
            "Converting to lowercase...\n",
            "--------------------------\n",
            "Removing html tags...\n",
            "--------------------------\n",
            "Removing nonword characters...\n",
            "--------------------------\n",
            "Removing stopwords...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text preprocessing completed.\n",
            "\n",
            "\n",
            "Preprocessing text on val set...\n",
            "\n",
            "\n",
            "Start text preprocessing: \n",
            "--------------------------\n",
            "Converting to lowercase...\n",
            "--------------------------\n",
            "Removing html tags...\n",
            "--------------------------\n",
            "Removing nonword characters...\n",
            "--------------------------\n",
            "Removing stopwords...\n",
            "Text preprocessing completed.\n",
            "\n",
            "\n",
            "Preprocessing text on test set...\n",
            "\n",
            "\n",
            "Start text preprocessing: \n",
            "--------------------------\n",
            "Converting to lowercase...\n",
            "--------------------------\n",
            "Removing html tags...\n",
            "--------------------------\n",
            "Removing nonword characters...\n",
            "--------------------------\n",
            "Removing stopwords...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text preprocessing completed.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# try different number of features\n",
        "#tv = TfidfVectorizer(max_df=1.0, min_df=0, max_features=40)\n",
        "#tv = TfidfVectorizer(max_df=1.0, min_df=0, max_features=100)\n",
        "#tv = TfidfVectorizer(max_df=1.0, min_df=0, max_features=200)\n",
        "#tv = TfidfVectorizer(max_df=1.0, min_df=0, max_features=1000)\n",
        "tv = TfidfVectorizer(max_df=1.0, min_df=0)\n",
        "\n",
        "tv.fit(x_train)\n",
        "x_train = tv.transform(x_train).toarray()\n",
        "x_val = tv.transform(x_val).toarray()\n",
        "x_test = tv.transform(x_test).toarray()\n",
        "\n",
        "vocab = tv.get_feature_names_out()\n",
        "\n",
        "print(pd.DataFrame(x_train, columns=vocab))\n",
        "print('TF-IDF vocabulary size: ', len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b5H2H8RvZBc",
        "outputId": "376fde57-90a9-40fc-8b76-5c6a9deef0e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        00  000  005  00am  00pm   01   02   03   05  098098  ...  zesty  \\\n",
            "0      0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0     0.0  ...    0.0   \n",
            "1      0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0     0.0  ...    0.0   \n",
            "2      0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0     0.0  ...    0.0   \n",
            "3      0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0     0.0  ...    0.0   \n",
            "4      0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0     0.0  ...    0.0   \n",
            "...    ...  ...  ...   ...   ...  ...  ...  ...  ...     ...  ...    ...   \n",
            "14995  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0     0.0  ...    0.0   \n",
            "14996  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0     0.0  ...    0.0   \n",
            "14997  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0     0.0  ...    0.0   \n",
            "14998  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0     0.0  ...    0.0   \n",
            "14999  0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  0.0     0.0  ...    0.0   \n",
            "\n",
            "       zeus  zion  zippy  zippys  ziti  zombie  zone  zoo  zulu  \n",
            "0       0.0   0.0    0.0     0.0   0.0     0.0   0.0  0.0   0.0  \n",
            "1       0.0   0.0    0.0     0.0   0.0     0.0   0.0  0.0   0.0  \n",
            "2       0.0   0.0    0.0     0.0   0.0     0.0   0.0  0.0   0.0  \n",
            "3       0.0   0.0    0.0     0.0   0.0     0.0   0.0  0.0   0.0  \n",
            "4       0.0   0.0    0.0     0.0   0.0     0.0   0.0  0.0   0.0  \n",
            "...     ...   ...    ...     ...   ...     ...   ...  ...   ...  \n",
            "14995   0.0   0.0    0.0     0.0   0.0     0.0   0.0  0.0   0.0  \n",
            "14996   0.0   0.0    0.0     0.0   0.0     0.0   0.0  0.0   0.0  \n",
            "14997   0.0   0.0    0.0     0.0   0.0     0.0   0.0  0.0   0.0  \n",
            "14998   0.0   0.0    0.0     0.0   0.0     0.0   0.0  0.0   0.0  \n",
            "14999   0.0   0.0    0.0     0.0   0.0     0.0   0.0  0.0   0.0  \n",
            "\n",
            "[15000 rows x 4901 columns]\n",
            "TF-IDF vocabulary size:  4901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import MaxPooling1D"
      ],
      "metadata": {
        "id": "RnubZ4JaDclj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_clinc150(x_train_cnn, y_train_cnn, batch_size, epochs, validation_data):\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(32, 3, activation='relu', input_shape = (x_train_cnn.shape[1],1)))\n",
        "  model.add(BatchNormalization()) \n",
        "  model.add(Dropout(0.5)) \n",
        "  model.add(MaxPooling1D(2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dense(150, activation='softmax'))\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer = 'adam', metrics=[\"accuracy\"])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "Q1KfqufkCv8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn_clinc150(x_train_cnn=x_train, y_train_cnn=y_train, batch_size=8, epochs=20, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXF9D-_SDLMx",
        "outputId": "1c3267a5-a4a6-4e13-c8a6-5541aa0e460b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_1 (Conv1D)           (None, 4899, 32)          128       \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 4899, 32)         128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4899, 32)          0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 2449, 32)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 78368)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               20062464  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 150)               38550     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,101,270\n",
            "Trainable params: 20,101,206\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=8, epochs=20, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KspTQPOBDhLg",
        "outputId": "9497feee-8e8a-423a-8312-d4f7814ea007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 362s 192ms/step - loss: 0.9552 - accuracy: 0.7869 - val_loss: 42.9386 - val_accuracy: 0.0420\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 360s 192ms/step - loss: 0.2439 - accuracy: 0.9350 - val_loss: 49.2352 - val_accuracy: 0.0191\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 359s 191ms/step - loss: 0.1639 - accuracy: 0.9547 - val_loss: 24.7657 - val_accuracy: 0.0433\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 358s 191ms/step - loss: 0.1477 - accuracy: 0.9598 - val_loss: 95.2635 - val_accuracy: 0.0269\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 349s 186ms/step - loss: 0.1225 - accuracy: 0.9677 - val_loss: 77.8555 - val_accuracy: 0.0289\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 350s 186ms/step - loss: 0.1206 - accuracy: 0.9691 - val_loss: 197.1566 - val_accuracy: 0.0118\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 351s 187ms/step - loss: 0.1067 - accuracy: 0.9735 - val_loss: 154.6620 - val_accuracy: 0.0111\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 349s 186ms/step - loss: 0.1041 - accuracy: 0.9715 - val_loss: 57.4617 - val_accuracy: 0.0487\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 350s 187ms/step - loss: 0.1006 - accuracy: 0.9763 - val_loss: 144.0267 - val_accuracy: 0.0129\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 354s 189ms/step - loss: 0.0999 - accuracy: 0.9751 - val_loss: 116.8470 - val_accuracy: 0.0222\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 349s 186ms/step - loss: 0.0945 - accuracy: 0.9770 - val_loss: 67.7843 - val_accuracy: 0.0158\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 350s 187ms/step - loss: 0.0935 - accuracy: 0.9785 - val_loss: 159.4115 - val_accuracy: 0.0251\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 349s 186ms/step - loss: 0.1024 - accuracy: 0.9771 - val_loss: 33.1054 - val_accuracy: 0.0858\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 353s 188ms/step - loss: 0.0917 - accuracy: 0.9777 - val_loss: 137.4693 - val_accuracy: 0.0162\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 365s 194ms/step - loss: 0.0882 - accuracy: 0.9791 - val_loss: 84.2984 - val_accuracy: 0.0313\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 365s 194ms/step - loss: 0.0982 - accuracy: 0.9785 - val_loss: 411.8066 - val_accuracy: 0.0191\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 365s 195ms/step - loss: 0.0860 - accuracy: 0.9798 - val_loss: 223.7210 - val_accuracy: 0.0358\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 364s 194ms/step - loss: 0.1010 - accuracy: 0.9778 - val_loss: 383.3012 - val_accuracy: 0.0173\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 362s 193ms/step - loss: 0.0963 - accuracy: 0.9789 - val_loss: 368.8388 - val_accuracy: 0.0158\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 364s 194ms/step - loss: 0.1009 - accuracy: 0.9785 - val_loss: 154.9991 - val_accuracy: 0.0931\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f15b7cbf3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}